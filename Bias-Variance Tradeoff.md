Theoretical results of statistics and Machine Learning is the fact that a model's generalization errors can be expressed as the sum of 3 erros

- Bias
	- This generalization error is due to the wrong assumptions. A high-bias model is most likely to underfit the training data
- Variance
	- This is caused due to the models sensitivity to small variations in the training data. A model with high degree of freedom is likely to have high variance and thus overfit the data
- Irreducible Error
	- This is due to the noisiness of the data. This can be handled by data cleaning

Increasing model complexity will increase variance and reduce bias.
Decreasing model complexity will increase bias and reduce variance.
Hence called Bias-Variance Tradeoff
