[[Attention is not all you need.pdf]]

Attention mechanism was initally developed to better learn long-range sequential knowledge, and found effective use in transformer networks.